{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7a726d8d-e110-4c47-88ca-292acb3f0d49",
   "metadata": {},
   "source": [
    "# Lab 6 - Data Splitting in Prediction\n",
    "STAT 450 \\\n",
    "TA: Gian Carlo (GC) \\\n",
    "Feb 14, 2025"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "749d3e2c-c257-4d0e-8e53-22796cb5a0c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preamble ###\n",
    "suppressPackageStartupMessages(library(tidyverse))\n",
    "suppressPackageStartupMessages(library(readr))\n",
    "suppressPackageStartupMessages(library(vip))\n",
    "suppressPackageStartupMessages(library(tidymodels))\n",
    "suppressPackageStartupMessages(library(readr))\n",
    "\n",
    "ggplot2::theme_set(theme_light())\n",
    "ggplot2::theme_update(text = element_text(size=25))\n",
    "options(repr.plot.width=15, repr.plot.height=7.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "398aca71-d48f-4a32-b27c-9b29aba042c8",
   "metadata": {},
   "source": [
    "## Background\n",
    "\n",
    "- Last Wednesday, \n",
    "you learned in class why you should split your data into training and test data sets \n",
    "to avoid double dipping when doing model selection for inference.\n",
    "- Today, we're going to see how this applies in a prediction setting.\n",
    "- In prediction, you want to maximize the prediction accuracy (appropriately measured) of your model.\n",
    "- If you use your whole sample to fit the model *and* estimate the prediction accuracy,\n",
    "you are going to (possibly severely) underestimate the out-of-sample prediction accuracy.\n",
    "    - This is because the model is fit to maximize the (in-sample) prediction accuracy,\n",
    "so you need to test your model on new data to estimate the out-of-sample prediction accuracy.\n",
    "- You can achieve this by splitting your data as before,\n",
    "fitting your model on the training data and then\n",
    "estimating the out-of-sample prediction accuracy using the test data.\n",
    "- But what happens if you also want to do model selection?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "689f3b06-95af-4766-875e-460280c08a3c",
   "metadata": {},
   "source": [
    "**Setting:** \n",
    "- You want to develop a model for prediction.\n",
    "- You currently have three competing models and want to choose the best one\n",
    "according to some measure of prediction accuracy.\n",
    "- You split your data into a training set and a test set and consider the following two approaches:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b85e526a-4f44-4891-baae-104fb4df4a12",
   "metadata": {},
   "source": [
    "**Approach 1:**\n",
    "1. We use the *training* set to fit the models.\n",
    "2. We select the model with the best prediction accuracy in the *training* set.\n",
    "3. We test the model in the *test* set to obtain an estimate of the out-of-sample error of the selected model.\n",
    "\n",
    "<p style=\"color: red;\">What do you think of this approach? </p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7055aeb-066c-430b-a588-645879e0b79d",
   "metadata": {},
   "source": [
    "**Approach 2**\n",
    "\n",
    "1. We use the *training* set to fit the models.\n",
    "2. We select the model with the best prediction accuracy in the *test* set.\n",
    "3. We use these results to obtain an estimate of the out-of-sample prediction accuracy of the selected model.\n",
    "\n",
    "<p style=\"color: red;\">What do you think of this approach? </p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de860249-14b5-4c59-9238-dd6732fbe561",
   "metadata": {},
   "source": [
    "### Solution: validation set\n",
    "\n",
    "- One way to address this shortcoming is to further split your data.\n",
    "- Specifically, the *validation set* is a second split of the training data.\n",
    "- Thus we end up with three data sets: (1) training set; (2) validation set; and (3) test set.\n",
    "\n",
    "1. We use the training set to fit as many models as we want.\n",
    "2. We select the model with the best prediction accuracy in the *validation* set (in-sample prediction accuracy).\n",
    "3. Once we decide which model is the best, we estimate the out-of-sample prediction accuracy using the *test* set."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f3add05-6459-4118-8ceb-55725959f19c",
   "metadata": {},
   "source": [
    "*Note:* This is not the only approach.\n",
    "E.g., you can also do $K$-fold cross-validation, \n",
    "where you resample your training data to create $K$ *folds*.\n",
    "For each fold and each model, \n",
    "you leave a part of the fold data out and use it as a validation set.\n",
    "\n",
    "- Benefits: for each model, you get multiple in-sample prediction measurements (uncertainty quantification!).\n",
    "- Cons: you have to fit every model $K$ times, which can be costly."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fdf1335-220d-4462-b8da-def7f34d3a1f",
   "metadata": {},
   "source": [
    "## Today: using the validation set approach with `tidymodels`\n",
    "\n",
    "![](https://www.tidymodels.org/start/case-study/img/hotel.jpg)\\\n",
    "\n",
    "We're going to be working with a data set from \n",
    "[Antonio, de Almeida, and Nunez (2019)](https://www.sciencedirect.com/science/article/pii/S2352340918315191?via%3Dihub).\n",
    "The data contain information about hotel bookings, \n",
    "such as which hotel the guests stay at, the type of room,\n",
    "the average daily rate, and whether the booking had any children.\n",
    "Check out the data dictionary \n",
    "[here](https://github.com/rfordatascience/tidytuesday/tree/main/data/2020/2020-02-11#data-dictionary).\n",
    "\n",
    "\n",
    "**Our goal:** create a model to predict how likely a booking is to contain children\n",
    "as a function of the other variables.\n",
    "\n",
    "Let's take a look at the data.\n",
    "\n",
    "*Note:* I borrowed very liberally (including the image above) from the `tidymodels` \n",
    "[predictive modelling case study](https://www.tidymodels.org/start/case-study/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b98f850c-8c72-4a56-a1a1-77c322f3faf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data import #### \n",
    "hotels <- read_csv(\"https://tidymodels.org/start/case-study/hotels.csv\") %>%\n",
    "  dplyr::mutate(across(where(is.character), as.factor)) \n",
    "\n",
    "dim(hotels)\n",
    "glimpse(hotels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "307daca4-7d9e-4efc-90b9-6ffb30827a64",
   "metadata": {},
   "source": [
    "Note that the data are severely unbalanced: most bookings didn't have any children!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3880647-b717-48c6-a03b-75588731a688",
   "metadata": {},
   "outputs": [],
   "source": [
    "hotels %>% \n",
    "  count(children) %>% \n",
    "  mutate(prop = n/sum(n))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae3e4b66-8342-459c-8b39-6b9f20fc6a61",
   "metadata": {},
   "source": [
    "### Creating the splits\n",
    "\n",
    "Below we create the training, validation, and test splits.\n",
    "We choose a 0.5, 0.25, 0.25 split.\n",
    "We use the `strata` option to ensure that the (uneven) split of children/no children\n",
    "is preserved in the splits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45111dbb-94ac-4762-9088-46bb8f4905f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create splits ####\n",
    "set.seed(450)\n",
    "splits <- initial_validation_split(hotels, \n",
    "                                   strata = children, \n",
    "                                   prop = c(0.5, 0.25)) # 50% training, 25% validation, 25% test\n",
    "\n",
    "hotel_training <- training(splits)\n",
    "hotel_test  <- testing(splits)\n",
    "hotel_val <- validation(splits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3169ef10-5711-453f-bb9c-d8eca5ae132d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check splits balances ####\n",
    "hotel_val  %>% \n",
    "  count(children) %>% \n",
    "  mutate(prop = n/sum(n))\n",
    "\n",
    "hotel_training %>% \n",
    "  count(children) %>% \n",
    "  mutate(prop = n/sum(n))\n",
    "\n",
    "hotel_test  %>% \n",
    "  count(children) %>% \n",
    "  mutate(prop = n/sum(n))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae6abf97-e384-49f6-a173-b0f599fd415b",
   "metadata": {},
   "source": [
    "### Defining the competing models\n",
    "\n",
    "For this case study,\n",
    "we are going to consider a logistic regression model.\n",
    "Let $L(\\beta;x,y)$ be the likelihood\n",
    "(which we covered in detail last week)\n",
    "for parameters $\\beta$, covariates $x$, and response $y$.\n",
    "Since we have a large number of covariates, \n",
    "we want to encourage our model to do some feature selection.\n",
    "We do this through an L1 penalty, or LASSO.\n",
    "In case you don't remember, instead of just minimizing the negative log-likelihood,\n",
    "we add a regularization term:\n",
    "$$\n",
    "\\mathrm{Cost}_\\lambda(\\beta) = -\\log L(\\beta;x,y) + \\lambda\\sum_p|\\beta_p|.\n",
    "$$\n",
    "\n",
    "**Our goal** is to decide is what penalty $\\lambda$ to use for the regularization term.\n",
    "We specify that below by setting `penalty = tune()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd88a310-3438-451e-b6b2-6bfa5c57fec0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define models to decide between ####\n",
    "lr_mod <- \n",
    "  logistic_reg(penalty = tune(), mixture = 1) %>% # We are tuning the penalty, ie, choosing between those models\n",
    "  set_engine(\"glmnet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fbdf7dc-ee63-4f80-8938-cbe3048b2d3b",
   "metadata": {},
   "source": [
    "And then we set a comprehensive grid of potential values for the penalty (in log scale):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a750c129-d1cb-49af-8c32-b30e4d8194b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define competing models ####\n",
    "lr_reg_grid <- tibble(penalty = 10^seq(-4, -1, length.out = 30))\n",
    "\n",
    "head(lr_reg_grid)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d98ee5ae-b9ba-470d-ab60-e40770bfc842",
   "metadata": {},
   "source": [
    "Below we create some data wrangling steps.\n",
    "This could be done using `tidyverse`&mdash;for today we are sticking with `tidymodels`\n",
    "so that you have some comprehensive code using the latter,\n",
    "but there are good arguments to stick with the former in practice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42919223-2cf9-4a75-b48d-68440c960f70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data wrangling with recipes ####\n",
    "holidays <- c(\"AllSouls\", \"AshWednesday\", \"ChristmasEve\", \"Easter\", \n",
    "              \"ChristmasDay\", \"GoodFriday\", \"NewYearsDay\", \"PalmSunday\")\n",
    "\n",
    "lr_recipe <- \n",
    "  recipe(children ~ ., data = hotels) %>%               # Children is the response\n",
    "  step_date(arrival_date) %>%                           # Arrival date is a date variable\n",
    "  step_holiday(arrival_date, holidays = holidays) %>%   # Dummy indicator for selected holidays (since they affect hotels)\n",
    "  step_rm(arrival_date) %>%                             # Remove arrival date (used for holidays only)\n",
    "  step_dummy(all_nominal_predictors()) %>%              # Creates dummy variables for nominal covariates\n",
    "  step_zv(all_predictors()) %>%                         # Removes covariates with single value (useless)\n",
    "  step_normalize(all_predictors())                      # Normalize all covariates\n",
    "\n",
    "\n",
    "# Create workflow ####\n",
    "lr_workflow <- \n",
    "  workflow() %>% \n",
    "  add_model(lr_mod) %>% \n",
    "  add_recipe(lr_recipe)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "863ebfc8-0482-4ca8-abac-9ff5a1c9c5f5",
   "metadata": {},
   "source": [
    "## Measuring prediction accuracy\n",
    "\n",
    "To measure prediction accuracy,\n",
    "we are going to be using the area under the \n",
    "Receiver Operating Characteristic, or [ROC](https://en.wikipedia.org/wiki/Receiver_operating_characteristic).\n",
    "This is a bit more comprehensive than simply using a pre-defined estimated probability threshold\n",
    "to classify new data; instead, it accounts for *all* possible probability thresholds.\n",
    "\n",
    "<div>\n",
    "<img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/1/13/Roc_curve.svg/1024px-Roc_curve.svg.png\" width=\"700\"/>\n",
    "</div>\n",
    "\n",
    "(Source: https://upload.wikimedia.org/wikipedia/commons/thumb/1/13/Roc_curve.svg/1024px-Roc_curve.svg.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69b49a03-c8c0-4021-9d6d-ac83f2c76d8a",
   "metadata": {},
   "source": [
    "### Tuning\n",
    "\n",
    "The code below:\n",
    "- fits each model (specified by the penalty) to the training data\n",
    "- and estimates the in-sample prediction accuracy using the validation data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb8afb93-6c26-4eaa-b1b7-e2bb0677320f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tune using validation data ####\n",
    "lr_res <- \n",
    "  lr_workflow %>% \n",
    "  tune_grid(validation_set(splits),\n",
    "            grid = lr_reg_grid,\n",
    "            control = control_grid(save_pred = TRUE),\n",
    "            metrics = metric_set(roc_auc))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecd7698c-3e71-4067-9977-dc11fae263f9",
   "metadata": {},
   "source": [
    "We can visualize the resulting AUC for each value of the penalty:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6df55c7d-afbb-4f28-bce3-e03f3891ceb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize tuning ####\n",
    "lr_plot <- \n",
    "  lr_res %>% \n",
    "  collect_metrics() %>% \n",
    "  ggplot() + \n",
    "  geom_point(aes(x = penalty, y = mean), size = 4) + \n",
    "  geom_line(aes(x = penalty, y = mean), linewidth = 1.5) + \n",
    "  labs(y = \"Area under the ROC Curve\") +\n",
    "  scale_x_log10(labels = scales::label_number())\n",
    "\n",
    "lr_plot "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eff9d815-ff1a-4b0e-9f1d-1f3924ef453a",
   "metadata": {},
   "source": [
    "and print out the top 15 models (by AUC):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc107b09-c777-4edb-bfc6-a31a3d6b78d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get best models ####\n",
    "top_models <-\n",
    "  lr_res %>% \n",
    "  show_best(metric = \"roc_auc\", n = 15) %>% \n",
    "  arrange(desc(mean)) \n",
    "top_models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d0b7157-e985-4c2b-96c1-1c66d5d92eaf",
   "metadata": {},
   "source": [
    "### Choosing the best model\n",
    "\n",
    "- Model 11 with a penalty of 0.001 and an AUC of ~0.876 is, empirically, the best one.\n",
    "- However, looking at the plot above, notice that almost all penalties $<0.01$\n",
    "produce a comparable AUC.\n",
    "- A larger LASSO penalty will induce more sparsity in the estimated coefficients\n",
    "when compared with a smaller penalty.\n",
    "- We want to encourage this behaviour because it will prevent nuisance variables from being included\n",
    "in the model, which might help prevent overfitting\n",
    "(and, if we were interested in inference, it would result in a more interpretable model).\n",
    "- Therefore, we might want to choose a larger penalty, say model 15 with a penalty of $0.002$\n",
    "(twice the penalty of the best model, and the 13th best one).\n",
    "- We highlight this penalty with its associated AUC below with a vertical line:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57508561-f6be-4dab-baf6-031e005077c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize again with chosen penalty ####\n",
    "lr_plot <- \n",
    "  lr_res %>% \n",
    "  collect_metrics() %>% \n",
    "  ggplot() + \n",
    "  geom_point(aes(x = penalty, y = mean), size = 4) + \n",
    "  geom_line(aes(x = penalty, y = mean), linewidth = 1.5) + \n",
    "  geom_vline(xintercept = top_models$penalty[13],\n",
    "            color = \"orange\", linewidth = 1.5) +\n",
    "geom_vline(xintercept = top_models$penalty[1],\n",
    "            color = \"blue\", linewidth = 1.5) +\n",
    "  labs(y = \"Area under the ROC Curve\") +\n",
    "  scale_x_log10(labels = scales::label_number())\n",
    "\n",
    "lr_plot "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7571b6ab-ac17-43ec-b4e8-7aefce340c69",
   "metadata": {},
   "source": [
    "### Comparison\n",
    "\n",
    "To further argue why we would not sacrifice much by choosing the larger penalty,\n",
    "below we save both penalties and then generate a plot with the entire ROCs plot together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db5f341e-a789-4759-b5f0-f1834dc4a8e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save our chosen best model ####\n",
    "lr_best <- \n",
    "  lr_res %>% \n",
    "  collect_metrics() %>% \n",
    "  arrange(desc(mean)) %>% \n",
    "  slice(13)\n",
    "lr_best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d76750ff-42c8-4e15-aed5-d3be6f7adf3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# And save empirical best model ####\n",
    "lr_best_empirical <- \n",
    "  lr_res %>% \n",
    "  collect_metrics() %>% \n",
    "  arrange(desc(mean)) %>% \n",
    "  slice(1)\n",
    "lr_best_empirical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffd37ab0-8d68-4b0d-8dda-233e62c0db39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot ROC curve ####\n",
    "lr_auc_best <- \n",
    "  lr_res %>% \n",
    "  collect_predictions(parameters = lr_best) %>% \n",
    "  roc_curve(children, .pred_children) %>% \n",
    "  mutate(model = \"Chosen model\")\n",
    "\n",
    "lr_auc_best_empirical <- \n",
    "  lr_res %>% \n",
    "  collect_predictions(parameters = lr_best_empirical) %>% \n",
    "  roc_curve(children, .pred_children) %>% \n",
    "  mutate(model = \"Empirical best\")\n",
    "\n",
    "bind_rows(lr_auc_best, lr_auc_best_empirical) %>% \n",
    "  ggplot(aes(x = 1 - specificity, y = sensitivity, col = model)) + \n",
    "  geom_path(lwd = 1.5, alpha = 0.8) +\n",
    "  geom_abline(lty = 3) + \n",
    "  coord_equal() + \n",
    "  scale_color_viridis_d(option = \"plasma\", end = .6)+\n",
    "  theme(legend.position = \"top\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b89930c-0332-46fd-b86f-888d1f51a41c",
   "metadata": {},
   "source": [
    "As we can see, both penalties result in nearly identical ROCs.\n",
    "We thus choose the one with the larger penalty as our best model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "527a69c4-176c-4f1d-bc65-509a2d69f6a5",
   "metadata": {},
   "source": [
    "### Estimating the out-of-sample prediction accuracy\n",
    "\n",
    "- Now we fit the selected model, this (last) time with the test data.\n",
    "- The resulting AUC is an out-of-sample estimate of the prediction accuracy of our model.\n",
    "- Ideally, it should be similar to the AUC of the validation data.\n",
    "Otherwise we should be concerned,\n",
    "since a much lower out-of-sample AUC could point to potential overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5118f21-7f47-4e8a-b391-aa526ae8b859",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit model last time ####\n",
    "last_lr_mod <-\n",
    "  logistic_reg(penalty = lr_best$penalty, mixture = 1) %>% \n",
    "  set_engine(\"glmnet\")\n",
    "\n",
    "# the last workflow\n",
    "last_lr_workflow <- \n",
    "  lr_workflow %>% \n",
    "  update_model(last_lr_mod)\n",
    "\n",
    "# the last fit\n",
    "last_lr_fit <- \n",
    "  last_lr_workflow %>% \n",
    "  last_fit(splits)\n",
    "\n",
    "last_lr_fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "244790b0-4683-424e-9c22-796f08303302",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot last ROC curve ####\n",
    "last_lr_fit %>% \n",
    "  collect_predictions() %>% \n",
    "  roc_curve(children, .pred_children) %>% \n",
    "  ggplot(aes(x = 1 - specificity, y = sensitivity)) + \n",
    "  geom_path(lwd = 1.5, alpha = 0.8) +\n",
    "  geom_abline(lty = 3) + \n",
    "  coord_equal()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b29cd77-1c63-425e-a12b-779365cabf13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# View metrics ####\n",
    "last_lr_fit %>% \n",
    "  collect_metrics()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab2857c6-0fa2-4357-8ddc-90b70a93879e",
   "metadata": {},
   "source": [
    "As we see, the out-of-sample AUC is very much in line with the in-sample one. Success!\n",
    "This is now a model that we can use for prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d12addc5-c73a-4da0-8127-eccf51278753",
   "metadata": {},
   "source": [
    "## Before you leave\n",
    "\n",
    "Think back to the two approaches that we discussed at the beginning.\n",
    "What are their problems (either overfitting or double dipping)?\n",
    "\n",
    "**Provide your answers on Canvas.**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "4.4.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
